---
title: "Project_2"
author: "Stephen Synchronicity"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
always_allow_html: yes
header-includes:
   - \usepackage{dcolumn}
output: 
  html_document: 
    self_contained: yes
    css: C:\Users\Stephen\Documents\R\win-library\3.4\neuhwk\rmarkdown\templates\report\resources\bootstrap.min.css
    highlight: zenburn
    keep_md: no
    theme: spacelab
    toc: yes
    toc_float: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE,warning=FALSE,cache=TRUE, fig.align='center', fig.height=5, fig.width=5, tidy=TRUE, tidy.opts=list(width.cutoff=80))
rmarkdown::html_dependency_jquery()
rmarkdown::html_dependency_bootstrap("spacelab")
rmarkdown::html_dependency_jqueryui()
set.seed(1)
options(scipen=12)
req.packages <- c("tidyverse","dplyr","htmltools","magrittr","ggmap","cluster","deldir","rgdal","broom")
for (q in seq_along(req.packages)) {
  suppressPackageStartupMessages(library(req.packages[q],character.only = T))
}
```
# Data Reading & Wrangling

```{r 'Load Data'}
Voters <- as.data.frame(readxl::read_xls(path="F:\\good voters 11-5-13 edited.xls",col_names = T))
nms <- Voters[1, ] %>% as.vector %>% unclass %>% unlist
nms[is.na(nms)] <- c("St Addr","Ward","Precinct")
names(Voters) <- nms
Voters <- Voters[-1,]
Voters$Zip <- Voters$Zip %>% gsub("^","0",.)
Voters <- Voters %>% tidyr::extract(Zip, into = c("Zip","ZipSuf"), "(\\d{5})(\\d{4})?", remove=FALSE)
Addresses <- unique(str_c(Voters$`St Addr`,Voters$`Residential Address - Street Name`,",","Waltham, MA",Voters$Zip,"USA",sep=" "))
```



## Geocoding Addresses
```{r 'Geocoding',eval=F}
# ----------------------- Thu Mar 08 22:49:26 2018 ------------------------#
# 2500 at a time
geo_replies <- vector("list",4820)
ctr <- rep(NA,4820)
for (i in retry) {
  ctr[i] <- i
  print(ctr[i])
  geo_replies[[i]]  <-  geocode(Addresses[i], output='all', messaging=TRUE, override_limit=TRUE,source = "dsk")
  
  print(geo_replies[[i]]$status)
  while(geo_replies[[i]]$status != "OK"){
    Sys.sleep(60)
    geo_replies[[i]]  <-  geocode(Addresses[i], output='all', messaging=TRUE, override_limit=TRUE,source = "dsk")
  }
}
answer <- data.frame(
  formatted_address=rep(NA,4820),
  neighborhood=rep(NA,4820),
  locality=rep(NA,4820),
  administrative_area_level_2=rep(NA,4820),
  administrative_area_level_1=rep(NA,4820),
  street_number=rep(NA,4820),
  route=rep(NA,4820),
  country=rep(NA,4820),
  postal_code=rep(NA,4820),
  postal_code_suffix=rep(NA,4820),
  lat=rep(NA,4820), 
  lon=rep(NA,4820), 
  accuracy=rep(NA,4820))
ct <- vector()
for (l in retry) {
  ct[l] <- l
  types <- geo_replies[[l]] %>% pluck(list("results",1,"address_components")) %>% lapply("[[","types") %>% lapply("[",1) %>% unlist %>% str_which("postal_code") 
  if(length(types)==1){next}
  for (c in seq_along(types)) {
    answer[l,types[c]] <- geo_replies[[l]] %>% pluck(list("results",1,"address_components",c,"short_name"))
  }
  answer[l,"lat"] <- geo_replies[[l]][["results"]][[1]][["geometry"]][["location"]][["lat"]]
  answer[l,"lon"] <- geo_replies[[l]][["results"]][[1]][["geometry"]][["location"]][["lng"]]
  answer[l,"formatted_address"] <- geo_replies[[l]][["results"]][[1]][["formatted_address"]]
  answer[l,"accuracy"] <- geo_replies[[l]][["results"]][[1]][["types"]][1]
}
# ----------------------- Fri Mar 09 11:08:25 2018 ------------------------#
# Determine items with missing data

type.v <- lapply(geo_replies[2522:4820],FUN=function(x){x %>% pluck(list("results",1,"address_components")) %>% lapply("[[","types") %>% lapply("[",1) %>% unlist %>% length}  )
which(type.v==1)
retry <- 2521+which(type.v==1)
sapply(types[2522:4820],length)
Addresses[retry]

retry <- which(answer$locality!="Waltham")
oql <- which(lapply(geo_replies,"[[","status")!="OK")
lengths <- vector()
for (l in seq_along(geo_replies)) {
  lengths[l] <- geo_replies[[l]] %>% pluck(list("results",1,"address_components")) %>% length
}
which.max(lengths)
write.csv(answer,"answer.csv")
save(geo_replies,file="geo_replies.Rdata")
```

### Joining with Original Data
```{r 'Join Geocoded Data'}
load("geo_replies.Rdata")
answer <- read.csv(file="answer.csv")
answer$postal_code %<>% gsub("^","0",.)
# ----------------------- Sun Mar 11 18:04:20 2018 ------------------------#
# Remove duplicates from answer
sum(duplicated(Voters$`Voter ID Number`))

names(answer)
names(Voters)


# ----------------------- Fri Mar 09 12:05:11 2018 ------------------------#
# Format Addresses the same
simpleCap <- function(x) {
  s <- strsplit(x, " ")[[1]]
  paste(toupper(substring(s, 1,1)), tolower(substring(s, 2)),
      sep="", collapse=" ")
}
Voters$`Residential Address - Street Name` %<>% sapply(.,simpleCap)

iVoters <- nrow(Voters)
formatted_address <- vector()
for (i in 1:iVoters) {
  formatted_address[i] <- str_c(Voters[i,'St Addr']," ",simpleCap(Voters[i,'Residential Address - Street Name']),","," Waltham, MA ",Voters[i,'Zip']," US",collapse="")
}
Voters$formatted_address <- formatted_address
ianswer <- nrow(answer)
formatted_address <- vector()
answer <- answer %>% mutate_at(vars(c('street_number','route','postal_code')),funs(as.character))
for (i in 1:ianswer) {
  formatted_address[i] <- str_c(answer[i,'street_number']," ",simpleCap(answer[i,'route']),","," Waltham, MA ",answer[i,'postal_code']," US",collapse="")
}
answer$formatted_address <- formatted_address
anyDuplicated(answer$formatted_address)


identical(class(answer$formatted_address),class(Voters$formatted_address))

Voters.full <- left_join(Voters,(answer %>% select(formatted_address,lat,lon) %>% unique),by="formatted_address")
# ----------------------- Fri Mar 09 12:37:03 2018 ------------------------#
# Fix the ones that were missed or duplicated
Voters.full.miss <- Voters.full[which(is.na(Voters.full$lat)),]
compareClasses <- sapply(Voters.full.miss,class) %>% data.frame(Colnames=names(.),Voters=.,stringsAsFactors = F) %>% rownames_to_column %>% left_join((as.data.frame(sapply(answer,class)) %>% rownames_to_column),by=c("Colnames"="rowname"))

by <- c("St Addr"="street_number","Residential Address - Street Name"="route","Zip"="postal_code")
answer$street_number %<>% as.numeric
Voters.full.miss <- left_join(Voters.full.miss[,c(1:13)],(answer %>% select(street_number,route,postal_code,lat,lon) %>% unique),by=by)
sum(is.na(Voters.full.miss$lat))
# No use
# ----------------------- Sun Mar 11 18:30:52 2018 ------------------------#
#

# Remove Any still missing
Voters.full  %<>%  filter(!is.na(lat)) %>% filter(!is.na(lon))
```

## Exploration
### Data Exploration
```{r 'Explore data'}
table(Voters.full$`Party Affiliation`)
qplot(Voters.full$`Party Affiliation`)
```

### Exploratory Graphs
```{r 'Map by Party'}
center <- c(lon=mean(Voters.full$lon),lat=mean(Voters.full$lat))
Waltham <- ggmap::get_googlemap(center=center, zoom=13, format="png8",maptype = "roadmap",scale=2)
ggmap(Waltham) +
  geom_point(data=Voters.full, mapping=aes(x=lon, y=lat, color=`Party Affiliation`),size=.5,na.rm=F)
```

## Campaign modeling
### k-Means for canvassing campaign drop points
```{r 'K-Means - D'}
# ----------------------- Fri Mar 09 17:16:24 2018 ------------------------#
# Using an iterative training approach to find the best kmeans model
alg <- c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen")
k <- c(1:15) # Choose based on the number of volunteers offering to help with canvassing
pars <- expand.grid(alg=alg,k=k,stringsAsFactors = F)
clusters <- vector("list",nrow(pars))
performance <- vector("list",nrow(pars))
D <- as.matrix(Voters.full %>% filter(`Party Affiliation`=="D") %>% select(c("lat","lon")))
for (a in seq_along(clusters)) {
  clusters[[a]] <- kmeans(D,centers=pars[a,"k"],nstart=20,algorithm = pars[a,"alg"])
  performance[[a]] <- clusterCrit::intCriteria(traj=D,part=clusters[[a]]$cluster,crit="all")
}
# ----------------------- Fri Mar 09 17:21:07 2018 ------------------------#
# Which model does the best according to the various criterion?
critNames <- lapply(performance[[1]],names) %>% names
bestIndex <- vector()
for (c in seq_along(critNames)) {
  bestIndex[c] <- clusterCrit::bestCriterion((lapply(performance,"[[",critNames[c]) %>% unlist),critNames[c])
}
(bestIndex.crit <- table(bestIndex))
# The model with 1 cluster. 
best <- names(bestIndex.crit) %>% as.numeric
pars[best,]
bestCl <- clusters %>% subset(seq_along(clusters)%in%best)
(dropPoints <- lapply(bestCl, "[[","centers") %>% unique)
# ----------------------- Fri Mar 09 17:53:32 2018 ------------------------#
# Visualize the D>rop points
# ----------------------- Sun Mar 11 19:36:43 2018 ------------------------#
# Add Voronoi diagram


dropPoints <- lapply(dropPoints, as.data.frame)
for (d in seq_along(dropPoints)) {
  D <- as.data.frame(D)
  if(length(dropPoints[[d]]$lon)>1){
    
    corners <-  c(range(D$lon),range(D$lat))
    voronoi <- deldir::deldir(dropPoints[[d]]$lon, dropPoints[[d]]$lat,rw=corners)
  print(ggmap(Waltham) +
  geom_point(data=D, mapping=aes(x=lon, y=lat),color="blue",size=.2,na.rm=F)+
  geom_point(data=dropPoints[[d]], mapping=aes(x=lon, y=lat),color="black")+
    geom_segment(
    aes(x = x1, y = y1, xend = x2, yend = y2),
    size = 1,
    data = voronoi$dirsgs,
    linetype = 1,
    color= "black"))}else {print(ggmap(Waltham) +
  geom_point(data=as.data.frame(D), mapping=aes(x=lon, y=lat),color="blue",size=.2,na.rm=F)+
  geom_point(data=dropPoints[[d]], mapping=aes(x=lon, y=lat),color="black"))}
}
```
<p> For a campaign, the number of volunteers available for canvassing can help determine the model to use for drop point selection, where the number of volunteers is closest to the number of centroids. Alternatively, the number of volunteers can be used to select the model based on the number of centroids.</p>
### Functionalize drop points algorithm
<strong>Inputs</strong>: 
<ol>
<li>data - voter data as data frame, f: factor on which to split the data for each clustering operation. Latitude and Longitude columns must be labeled lat and lon respectively and the factor variable name to split the data on must corresponding exactly to the character vector for f.</li>
<li>f - a character vector equivalent to the name of the factor variable on which to split the data (typically party affiliation.)</li>
<li>k: k (centroids) an integer vector of the number of possible drop points.</li>
<li><em>Optional</em> nms: The names of the factor levels to compute drop points for (party affiliation abbvs typically).</li>
<li><em>Optional</em> best.clust: A logical indicating the best clusters are requested in the output, if FALSE, all the clusters are included in the output.</li>
</ol>
 <strong>Outputs</strong>: a list, with sublists for each factor level, each with 3 sublists for each unique factor level. 
<ol>
<li>[['BestConfigs']] contains the parameters for the best configurations of kmeans. </li>
<li>[['DropCoords']] - a list of the drop points corresponding to each of the best cluster models.</li>
<li>[['Clusters']] - a list of the highest performing cluster models. If best.clust=FALSE, a list containing [['clusters']] - all cluster models and [['performance']] - the respective performance of each cluster across numerous criteria. (See ?clusterCrit for details on performance criteria)</li>
</ol>
```{r 'dropPoints - Cluster based on factor'}
f <- "Party Affiliation"
data <- Voters.full
# ----------------------- Fri Mar 09 17:56:06 2018  ------------------------#
# Kmeans for drop points sorted by factor
dropPoints <- function(data,f,k=c(1:10),nms=NULL,best.clust=T) {
# ----------------------- Fri Mar 09 18:12:36 2018 ------------------------#
# Inputs: data - voter data as data frame, f: factor on which to split the data for each clustering operation, k: k (centroids) a vector of the number of possible drop point.nms: The names of the factor levels to compute drop points for (party affiliation abbvs typically). best.clust: A logical indicating the best clusters are requested in the output, if FALSE, all the clusters are included in the output.
# Outputs: a list, with sublists for each factor level, each with 3 sublists for each unique factor level. 
#[[BestConfigs]] contains the parameters for the best configurations of kmeans. 
# [['DropCoords']] - a list of the drop points corresponding to each of the cluster models
# [['Clusters']] - a list of the highest performing cluster models.If best.clust=FALSE, a list containing [['clusters']] - all cluster models and [['performance']] - the respective performance of each cluster across numerous criteria. (See ?clusterCrit for details on performance criteria)
  if(is.null(nms)){nms <- unique(data[[f]])} 
  ko <- k
  drops <- vector("list",length(nms))
  names(drops) <- nms
  alg <- c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen")
  pars <- expand.grid(alg=alg,k=k,stringsAsFactors = F)
  # Begin party loop
  for (n in seq_along(nms)) {
    k <- ko
    clusters <- vector("list",nrow(pars))
    performance <- vector("list",nrow(pars))
    df.f <- data %>% subset(.[[f]]==nms[n]) %>% select(c("lat","lon"))
    df <- scale(df.f)
    if(nrow(df.f) < max(k)){k <- c(1:(nrow(df.f)-1))
    pars <- expand.grid(alg=alg,k=k,stringsAsFactors = F)
    clusters <- vector("list",nrow(pars))
    performance <- vector("list",nrow(pars))}
    # Begin cluster loop
    for (a in seq_along(clusters)) {
    clusters[[a]] <- kmeans(df,centers=pars[a,"k"],nstart=25,algorithm = pars[a,"alg"],iter.max=25)
    performance[[a]] <- clusterCrit::intCriteria(traj=df,part=clusters[[a]]$cluster,crit="all")
    }#End clustering loop
    critNames <- lapply(performance[[1]],names) %>% names %>% as.list
names(critNames) <- critNames %>% unlist
    for (c in seq_along(critNames)) {
     critNames[[c]] <- performance %>% lapply(FUN="[[",critNames[[c]]) %>% unlist %>% clusterCrit::bestCriterion(crit=critNames[[c]])
    }
    (bestIndex.crit <- table(critNames %>% unlist))
    best <- names(bestIndex.crit) %>% as.numeric
    
    bestCl <- clusters %>% subset(seq_along(clusters)%in%best)
    matCl <- lapply(bestCl, "[[","cluster")
    dropPts <- vector("list",length(matCl))
    for (m in seq_along(matCl)) {
    dropPts[[m]]  <- df.f %>% mutate(Cl=matCl[[m]]) %>% group_by(Cl) %>% summarize_all(funs(mean))
    }
    if(best.clust==T){drops[[n]] <- list(BestConfigs=pars[best,],DropCoords=dropPts,Clusters=bestCl)}else {
      drops[[n]] <- list(BestConfigs=pars[best,],DropCoords=dropPts,Clusters=list(clusters=clusters,performance=performance))
    }
    
  }#End party loop
  
  return(drops)
}

drops <- dropPoints(data=data,f="Party Affiliation")
```

### Visualizing Centers

```{r 'visDrops Function for Visualizing Centers'}
# ----------------------- Fri Mar 09 17:53:32 2018 ------------------------#
# Visualize the Drop points

visDrops <- function(data=data,f=f,drops=drops) {
  c <- RColorBrewer::brewer.pal(length(drops),"Set1")
  for (d in seq_along(drops)) {
  df.f <- data %>% subset(.[[f]]==names(drops)[d]) %>% select(c("lat","lon"))
  Waltham <- get_googlemap(center=c(mean(df.f$lon),mean(df.f$lat)), zoom=13, format="png8",maptype = "roadmap")
  co <- c[d]
    for (i in seq_along(drops[[d]][['DropCoords']])) {
# ----------------------- Mon Mar 12 13:13:54 2018 ------------------------#
      # Add gridarrangement to output
      ld <- length(drops[[d]][['DropCoords']])
      grob.list <- vector("list",ld)
      gcol <- c(2,3,4)
      gcol <- gcol[max(which(ld %% gcol ==0|ld %% gcol ==1))]
      
      ld / gcol
      t <- paste("Factor=",names(drops)[d],", k=",drops[[d]][["BestConfigs"]][["k"]][i],sep="")
      centers <- drops[[d]][['DropCoords']][[i]]
      if(length(centers$lon)>1){
    corners <-  c(range(df.f$lon),range(df.f$lat))
    voronoi <- deldir::deldir(centers$lon, centers$lat,rw=corners)
    print(ggmap(Waltham) +
    geom_point(data=df.f, mapping=aes(x=lon, y=lat),color=co,size=.6,na.rm=F)+
    geom_point(data=centers, mapping=aes(x=lon, y=lat),color="black")+
      geom_segment(
    aes(x = x1, y = y1, xend = x2, yend = y2),
    size = 1,
    data = voronoi$dirsgs,
    linetype = 1,
    color= "black")+
      labs(title = t,
      subtitle = "",
      caption = "",
      x = "",y = "") +
      theme(plot.title = element_text(hjust = .5),plot.subtitle = element_text(hjust = .5),axis.text = element_blank()))
    ggplot2::ggsave(filename=paste(t,".pdf",sep=""),plot=last_plot(), path="Plots",device = "pdf",width = 5, height = 5, units = "in")
      }else {
      print(ggmap(Waltham) +
    geom_point(data=df.f, mapping=aes(x=lon, y=lat),color=co,size=.6,na.rm=F)+
    geom_point(data=centers, mapping=aes(x=lon, y=lat),color="black")+
      labs(title = t,
      subtitle = "",
      caption = "",
      x = "",y = "") +
      theme(plot.title = element_text(hjust = .5),plot.subtitle = element_text(hjust = .5),axis.text = element_blank(),axis.ticks = element_blank()
      ))
        ggplot2::ggsave(filename=paste(t,".pdf",sep=""),plot=last_plot(), path="Plots",device = "pdf",width = 5, height = 5, units = "in")
    }
      
    }
  } 
}
visDrops(data=data,f=f,drops=drops)
```
<p class="a">A kmeans object from the dropPoints output is to be selected manually based on the number of volunteers available. This cluster data is then used to group the original data.</p>

```{r 'Group by Clusters'}
# ----------------------- Sat Mar 17 16:36:52 2018 ------------------------#
# Cluster the data, graph to ensure arruracy


D.grpd <- data %>% filter(`Party Affiliation` =="D") %>% cbind(Group=drops[["D"]][["Clusters"]][[6]]$cluster)
D.grpd$Group %<>% factor 
ggmap(Waltham)+
  geom_point(data=D.grpd,mapping=aes(x=lon,y=lat,color=Group),size=1)

```
<strong>startPoints</strong>
<p class="a">
A function that finds startings points at the center of density for the area to ensure the time available canvassing captures the greatest number of voters.<p>
```{r 'Locate the starting point'}
# ----------------------- Sat Mar 17 16:28:04 2018 ------------------------#
# Inputs: data - original data, grp - variable name for the cluster group value
# Depends: stringr
startPoints <- function(data,grp) {
 var.num <- names(data)[sapply(data,is.numeric)]
 lalogrp <- c(stringr::str_extract(var.num,regex("(?:lat?i?t?u?d?e?)",ignore_case=T)) %>% .[!is.na(.)],stringr::str_extract(var.num,regex("(?:lon?g?i?t?u?d?e?)",ignore_case=T)) %>% .[!is.na(.)],grp)
 d <- data[,lalogrp]

 grps <- unique(d[,grp])
 sPs <- sapply(grps, function(g,d,grp){
# ----------------------- Sat Mar 17 15:55:34 2018 ------------------------#
# For each point, determine the number of points within 1 SD, the point with the most other points within radius wins.
   d.grp <- d[d[,grp]==g,]
   d.grp.sd <- sapply(d.grp[,lalogrp[1:2]],sd)
d.grp.nn <- apply(d.grp[,1:2],1,FUN = function(r,d.grp.sd,d.grp,lalogrp){
  nn <- intersect(which(
    d.grp[,lalogrp[1]]>r[1]-d.grp.sd[1] & d.grp[,lalogrp[1]]<r[1]+d.grp.sd[1]
    ),
    which(
      d.grp[,lalogrp[2]]>r[2]-d.grp.sd[2] & d.grp[,lalogrp[2]]<r[2]+d.grp.sd[2]
      )) %>% length # Find which (the index of) values that are within one sd of each point in lat and in lon respectively, and return the intersection (IE individual lat/lon points). Return the length of this vector as the number of nodes it's nearest too.
},d.grp.sd=d.grp.sd, d.grp=d.grp, lalogrp=lalogrp) # Apply over rows outputs the number of points each point is nearest too 
d.grp[which.max(d.grp.nn),] # Returns the index of the point with the most nearest neighbors. 
 },d=d,grp=grp,USE.NAMES=T,simplify=T) # Returns the starting points for each group 
 sPs <- as.data.frame(matrix(data=sPs %>% unlist,ncol=3,byrow=T,dimnames = list(rows=NULL,cols=lalogrp)))
 return(sPs)
}

sPs <- startPoints(D.grpd,"Group")

meters <- c(lat=111080.83603511854,lon=111577.12826935215) # Meters per unit latitude,longitude
```

<strong>Route Optimization - Function: routes() </strong>
<p>The function expects data with a column entitled "Group" (or specify with grp argument) that contains the cluster number the observation belongs to. The data is filtered by group. The function then uses an iterative k-nearest neighbors from the spdep package to provide an index of "as the crow flies" nearest neighbors for each point observation in the cluster. The starting points sPs object is used to select the point with the highest number of nearest neighbors within one standard deviation to start the route data frame (route). The nearest neighbors indexes for this point (d.nn columns: nnX) are used to subset the cluster to just the k-nearest neighbors as potential destinations (nn). K defaults to 5 and increments up if all k nearest neighbors are already in the route, the overall nearest neighbors index object (d.nn) is then subsetted to include only points not already in the route before recalculating nearest neighbors indexes. The nearest neighbors (nn) are used to minimize the number of requests made on the gmapsdistance API, as a standard plan API key has <a href="https://developers.google.com/maps/documentation/javascript/usage">rate limits</a>. The return from gmapdistance is used to select the point with the least walking time as the next destination that is added to the route. The next iteration of loop starts with this point. The outcome (provided no errors are encountered), is a list of data frames, one for each cluster, with a list of the unique points in the cluster sorted into a shortest path walking route indicated by the Index column. The first point will be the optimal point to start so as to maximize the number of canvassed houses in the least amount of time. The data frames can be joined with the original data to include the full data for each observation sorted by factor. The data is then filtered by group to form new dataframes for each of the groups. Each group dataframed is sorted by index for optimal route, and then provided to the canvasser.   
</p>

```{r 'Route Optimization'}
# ----------------------- Sat Mar 17 12:14:26 2018 ------------------------#
# https://cran.r-project.org/web/packages/gmapsdistance/README.html
# https://developers.google.com/maps/documentation/javascript/usage
#install.packages("gmapsdistance")
library(gmapsdistance)
library(sp)
key <- "AIzaSyB7spZTdy-yEt8qUe089twfvKRIvx738Q4" #Maps API Key
set.api.key(key)
get.api.key()


# ----------------------- Sat Mar 17 15:55:07 2018 ------------------------#
# Misadventure in attempting to locate the center of density

# sp::bbox(SpatialPoints(g1))
# g1.poly  <-  SpatialPolygons(list(Polygons(list(Polygon(g1)), ID = "a")), proj4string=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"))
# plot(, axes = TRUE)
# g1.area <-  SpatialPolygons(list(Polygons(list(),1)))
# g1.area %<>% SpatialPolygonsDataFrame(g1.ahull)
# g1.r <- raster::raster(g1.area)
# raster::res(g1.r) <- raster::area(g1.area)/mean(g1.sd)
# g1.r.a <- raster::rasterize(g1.area,g1.r)
# g1.quads <- as(`g1.r.a`, 'SpatialPolygons')

# points(g1, col='red', cex=.5)
# ----------------------- Sat Mar 17 21:35:51 2018 ------------------------#
# Function: routes
# Input: sPs - the starting points data frame, data - the original data filtered by factor to which the cluster groupings correspond, with a column added with the group labels. grp - the name of this column
# Output: A list of dataframes for each group with a Route column that has the index numbers used to sort the order from point to point of the walking route. 
# Depnends: spdep, gmapsdistance, tidyverse
routes <- function(sPs, data, grp="Group", key=NULL, verbose=F){
  if(is.null(key)){key <- gmapsdistance::get.api.key()}
  sPs <- sPs[order(sPs$Group),]#
# ----------------------- Sun Mar 18 20:18:13 2018 ------------------------#
# TODO(Add a repeating d.knn loop that filters re-sorts each time)

# ----------------------- Sat Mar 17 19:55:56 2018 ------------------------#
# Needs to loop and create an ordered vector of observation indices indicating a sequence to follow. 
out <- apply(sPs,1,function(sP,data,grp,key){
  lalogrp <- names(sP) # Grab the col names
  d.grp <- data[data[[grp]]==sP[,"Group",drop=T],lalogrp] %>% subset(subset=!duplicated(d.grp[,1:2])) # Subset based on group number and filter duplicates
  k <- 5
  nn.nms <- c(paste0("nn",1:k))
  d.knn <- spdep::knearneigh(as.matrix(d.grp[,1:2]),k=k,longlat=T) %>% spdep::knn2nb(sym=F) %>% unlist %>% matrix(ncol=k,byrow=T,dimnames=list(rows=names(.),cols=nn.nms)) %>% unclass %>% cbind(d.grp,.)
  # Find the 5 nearest neighbors by lat,lon distance for each observation
  # Translate the results into a matrix
  # Combine it with the original data
  
  
# ----------------------- Sat Mar 17 22:06:02 2018 ------------------------#
# Use gmapsdistance to iteratively compute the shortest path walking distance between all points 
 #route.debug <- route # save route just in case it doesnt work right
  route <- data.frame(sP,Index=1)
  len <- nrow(route):(nrow(d.grp))
  for (i in len) {
    route <- route[!is.na(route[,1]),]
    if(i==2){r <- i-1}else {r <- nrow(route)
    i <- r+1} # Row counter for the gmaps input
    if(verbose==T){print(r);print(k)}
    
     
    if(any(d.knn[[lalogrp[1]]]==route[-r,1,drop=T]&d.knn[[lalogrp[2]]]==route[-r,2,drop=T])){ print("Removing vars from d.knn: 1")
      d.knn <- d.knn[(d.knn[[lalogrp[1]]] %in% route[-r,1,drop=T] & d.knn[[lalogrp[2]]] %in% route[-r,2,drop=T])==F,] %>% subset(subset=!is.na(.[[lalogrp[1]]])) %>% unique} # Remove all nearest neighbors that have already been added to the route
    
if(all((route[r,1] %in% d.knn[,1] & route[r,2] %in% d.knn[,2]))==F | length((d.knn[,1]==route[r,1] & d.knn[,2]==route[r,2]))==0){
  print("If there are not knn index matches, resort d.knn")
   nn.nms <- c(paste0("nn",1:k))
  d.rem <- d.grp[!d.grp[,1] %in% route[-r,1] & !d.grp[,2] %in% route[-r,2],1:2] # Remaining obs
  d.knn <- d.rem %>% as.matrix %>% spdep::knearneigh(.,k=k,longlat=T) %>% spdep::knn2nb(sym=F) %>% unlist %>% matrix(ncol=k,byrow=T,dimnames=list(rows=names(.),cols=nn.nms)) %>% unclass %>% cbind(d.rem[,1:2],.)
}
# ----------------------- Sun Mar 18 15:59:56 2018 ------------------------#
# Subset nn based on the last observed value in route
d.knn.lastobs <- d.knn[d.knn[,1]==route[r,1] & d.knn[,2]==route[r,2],] # The row of the nearest neighbors that is equivalent to the last obs
   nn <- d.knn.lastobs[,nn.nms,drop=T] %>% unlist %>% d.knn[.,] # The Indexes of nn are used to subset nn
  nn <- nn %>% subset(subset=((nn[,1] %in% route[,1] & nn[,2] %in% route[,2])==F)) %>% .[!is.na(.[,1]),]
  print.AsIs(nn)
  
  if(nrow(nn)==1 & nn[1,1] > 2){route[(r+1),] <- c(nn[,1:2],sP["Group"],Index=r+1)
       next}
  
  if(nrow(nn) < 1){
# ----------------------- Sun Mar 18 01:00:42 2018 ------------------------#
# TODO(Debug the recalculation of nearest neighbors)
  d.knn <- d.knn[(d.knn[[lalogrp[1]]] %in% route[-r,1,drop=T] & d.knn[[lalogrp[2]]] %in% route[-r,2,drop=T])==F,]
         
  if(nrow(d.knn)>k & k<16){k <- k+1}else if(nrow(d.knn)<=k){k <- (nrow(d.knn)-1)}#Evaluate k based on rows left in nearest neighbors df
  
  nn.nms <- c(paste0("nn",1:k))
  d.rem <- d.grp[!d.grp[,1] %in% route[-r,1] & !d.grp[,2] %in% route[-r,2],1:2] # Remaining obs
  d.knn <- d.rem %>% as.matrix %>% spdep::knearneigh(.,k=k,longlat=T) %>% spdep::knn2nb(sym=F) %>% unlist %>% matrix(ncol=k,byrow=T,dimnames=list(rows=names(.),cols=nn.nms)) %>% unclass %>% cbind(d.rem[,1:2],.)
  
         next}
   o <- paste(route[r,1,drop=T],route[r,2,drop=T],sep="+")
   d <- apply(nn,1,function(x){
     paste(x[1],x[2],sep="+")
          }) # o & d are formatting origin and destination for the gmaps call
# ----------------------- Sat Mar 17 19:34:07 2018 ------------------------#
# Retrieve Distance Matrix as list of DFs
   if(length(d)==0){next}
     wlks <- gmapsdistance::gmapsdistance(origin = o,destination = d,combinations = "all",mode = "walking", key=key, shape="long")
 # Select the next point based on the output from gmapsdistance
    node <- c(unlist(nn[which.min(wlks$Time$Time),1:2,drop=T]),sP["Group"],Index=(r+1)) 
    if(verbose==T){print(node)}
    if(any(is.na(node))){warning(paste("Error on point:",r,"returning object for restart"))
    route  <<- route
      return("Route returned to global variable")}
    route[(r+1),] <- node
    }
  },data=data,grp=grp,key=key)
return(out)
}
Routes <- routes(sPs=sPs, data=data, key=key) #UNtested
```

### Visualizing Route

```{r 'Visualizing Route'}
# ----------------------- Sun Mar 18 08:45:56 2018 ------------------------#
# TODO(Iteratively graph segments on map set to extent  of values in group)
# https://stackoverflow.com/questions/15987367/how-to-add-layers-in-ggplot-using-a-for-loop
# ----------------------- Sun Mar 18 14:23:52 2018 ------------------------#
# 1. Create map of each area
# 2. Iteratively graph segments with arrows indicating direction.
# 3. Label starting point and ending point
center <- c(lon=mean(route$lon),lat=mean(route$lat))
map.grp <- ggmap::get_googlemap(center=center, zoom=15, format="png8",maptype = "roadmap",scale=2)
ggmap(map.grp)+
  geom_point(data=route,mapping=aes(x=lon,y=lat),color="blue",size=.75)+
  geom_path(data=route,mapping=aes(x=lon,y=lat),color="red",arrow= arrow(length = unit(3,"mm")))
```

### Hierarchical Clustering for Variance Minimization

<p><a href="http://uc-r.github.io/hc_clustering" target="_blank">HC Clustering.</a> The results are unimpressive. k-Means is more effective at partitioning the data in a logical fashion.</p>
```{r 'Clustering for min Variance', echo=TRUE,eval=F}
data <- Voters.full
hc.dropPoints <- function(data,f,c=NULL,verbose=F) {
  nms <- unique(data[[f]])
  drop.areas <- vector("list",length(nms))
  names(drop.areas) <- nms
  var.num <- names(data)[sapply(data,is.numeric)]
  lalo <- c(stringr::str_extract(var.num,regex("(?:lat?i?t?u?d?e?)",ignore_case=T)) %>% .[!is.na(.)],stringr::str_extract(var.num,regex("(?:lon?g?i?t?u?d?e?)",ignore_case=T)) %>% .[!is.na(.)])
  #Begin party loop
  for (m in seq_along(nms)) {
    df.f <- (data %>% subset(subset=(.[[f]]==nms[m])) -> df.s) %>% select(c("lat","lon")) %>% stats::dist(method= "euclidean")
   # Dissimilarity matrix
    if(nrow(df.s) < 10){drop.areas[[nms[m]]] <- "Under 10 observations, no clustering necessary"
      next}
    if(is.null(c)){
      c <- 2:10
    runs <- expand.grid(method=c("ward.D","ward.D2","average"),
                c=c)}else {runs <- expand.grid(method=c("ward.D","ward.D2","average"),
                c=c)}
    hcs <- vector("list",nrow(runs))
    S <- data.frame(S=seq(nrow(runs)),Index=seq(nrow(runs)),stringsAsFactors = F)
    for(r in 1:nrow(runs)){
      if(verbose==T){print.AsIs(runs[r,])
      cat(nrow(df.s))}
      hc <- hclust(df.f, method = runs[r,1])
      hcs[[r]] <- dendextend::cutree(hc,runs[r,2])
      S[r,1] <-  summary(silhouette(hcs[[r]],df.f))$si.summary[3]
    }
    hcs.best <- hcs[S[order(S$S,decreasing = T),][['Index']][1:5]]
    names(hcs.best) <- 1:5
   hcs.df <- lapply(hcs.best,FUN=function(x,df.s){
     df <- data.frame(Rowname=rownames(df.s),lat=df.s[[lalo[1]]],lon=df.s[[lalo[2]]],Group=x %>% as.factor)
     return(df)
   },df.s=df.s)
   names(hcs.df) <- sapply(hcs.best,FUN=function(x){x %>% unique %>% length},simplify = F,USE.NAMES = T) %>% paste0(names(.),"-","Cuts:",.)
     
    drop.areas[[nms[m]]] <- hcs.df
    
  }# End loops
  
return(drop.areas)
}

hc.drops <- hc.dropPoints(data=data,f=f,verbose=T)

ggmap(Waltham)+
  geom_point(data= hc.drops$D$`3-Cuts:10`,mapping = aes(x=lon,y=lat,color=Group),size=.75)+
  scale_color_brewer(type="qual",palette = 2)
```

## Visualizing Demographics

### Load Data
```{r 'Income Data by Block Group', echo=F,eval=F}
ds <- c("B19013")
yr <- c(thirteen='13',sixteen='16')

files <- expand.grid(ds=ds,yr=yr,stringsAsFactors = F)
`13.li` <- vector("list",1)
names(`13.li`) <- ds
`16.li` <- vector("list",1)
names(`16.li`) <- ds
ds.li <- list('thirteen'=`13.li`,'sixteen'=`16.li`)

for (i in 1:nrow(files)) {
  names(yr)
  ds.li[[names(files[i,'yr'])]][[files[i,'ds']]]$description <- read_lines(paste("C:\\Users\\Stephen\\Documents\\Northeastern\\Git\\ppua5302\\Project 2\\Waltham\\CensusBlocks\\ACS_",files[i,'yr'],"_5YR_",files[i,'ds'],".txt",sep=""),n_max=2)
  meta <- paste("C:\\Users\\Stephen\\Documents\\Northeastern\\Git\\ppua5302\\Project 2\\Waltham\\CensusBlocks\\ACS_",files[i,'yr'],"_5YR_",files[i,'ds'],"_metadata.csv",sep="")
  ds.li[[names(files[i,'yr'])]][[files[i,'ds']]]$meta <- read.csv(meta)
  data <- paste("C:\\Users\\Stephen\\Documents\\Northeastern\\Git\\ppua5302\\Project 2\\Waltham\\CensusBlocks\\ACS_",files[i,'yr'],"_5YR_",files[i,'ds'],"_with_ann.csv",sep="")
  ds.li[[names(files[i,'yr'])]][[files[i,'ds']]]$data <- read.csv(data)
}
attributes(ds.li[["thirteen"]][["B19013"]][["data"]])$subheaders <- ds.li[["thirteen"]][["B19013"]][["data"]][1,] %>% unclass %>% unlist %>% as.character()
ds.li[["thirteen"]][["B19013"]][["data"]] <- ds.li[["thirteen"]][["B19013"]][["data"]][-1,]
attributes(ds.li[["sixteen"]][["B19013"]][["data"]])$subheaders <- ds.li[["sixteen"]][["B19013"]][["data"]][1,] %>% unclass %>% unlist %>% as.character()
ds.li[["sixteen"]][["B19013"]][["data"]] <- ds.li[["sixteen"]][["B19013"]][["data"]][-1,]
```


```{r 'Load Block Groups', echo=TRUE}
censusblocks <- read.csv(file="C:\\Users\\Stephen\\Documents\\Northeastern\\Git\\ppua5302\\Project 2\\Waltham\\CensusBlocks.csv")
CensusBG <- c("C:\\Users\\Stephen\\Documents\\Northeastern\\Git\\ppua5302\\Project 2\\Waltham\\CensusBlocks\\Census 2010 Block Groups\\GISDATA_CENSUS2010BLOCKGROUPS_POLY.shp")
CensusBG<- rgdal::readOGR(dsn=CensusBG)
CensusBG.lola <- spTransform(CensusBG, CRS("+proj=longlat +datum=WGS84"))
CensusBG.df <- broom::tidy(CensusBG.lola)
xml2::read_xml("C:\\Users\\Stephen\\Documents\\Northeastern\\Git\\ppua5302\\Project 2\\Waltham\\Map_2018031122372932.kmz")
```
```{r 'Subset and Merge Demographic Data'}
# ----------------------- Mon Mar 12 09:39:38 2018 ------------------------#
# Subset spatial polygons dataframe
cg <- c(as.character(ds.li[["thirteen"]][["B19013"]][["data"]][["GEO.id2"]]),as.character(ds.li[["sixteen"]][["B19013"]][["data"]][["GEO.id2"]]))
cg <- unique(cg)
CensusBG.Waltham <- CensusBG.lola[CensusBG.lola@data$GEOID10 %in% cg,]
CensusBG.Waltham.df <- broom::tidy(CensusBG.Waltham,region='GEOID10')

# ----------------------- Mon Mar 12 09:39:51 2018 ------------------------#
# Merge census data
Cen13.Waltham <- ds.li[["thirteen"]][["B19013"]][["data"]][ds.li[["thirteen"]][["B19013"]][["data"]][["GEO.id2"]] %in% CensusBG.lola@data$GEOID10,]
Cen16.Waltham <- ds.li[["sixteen"]][["B19013"]][["data"]][ds.li[["sixteen"]][["B19013"]][["data"]][["GEO.id2"]] %in% CensusBG.lola@data$GEOID10,]
CensusBG.W.Inc <- left_join(CensusBG.Waltham.df,(Cen13.Waltham %>% select(HD01_VD01,GEO.id2)),by=c("id"="GEO.id2")) %>% rename("Income13"="HD01_VD01")
CensusBG.W.Inc <- left_join(CensusBG.W.Inc,(Cen16.Waltham %>% select(HD01_VD01,GEO.id2)),by=c("id"="GEO.id2")) %>% rename("Income16"="HD01_VD01")
CensusBG.W.Inc$Income13 <- CensusBG.W.Inc$Income13 %>% as.character %>% as.numeric()
CensusBG.W.Inc$Income16 <- CensusBG.W.Inc$Income16 %>% as.character %>% as.numeric()
TaxBrackets <- xml2::read_html("https://taxfoundation.org/2017-tax-brackets/") %>% rvest::html_node(xpath="//*[@id='post-content']/table[2]") %>% rvest::html_table()

```
```{r 'Get Polling Locations'}
pp <- xml2::read_html("https://www.city.waltham.ma.us/elections/pages/polling-places") %>% rvest::html_node(xpath="//*[@id='block-system-main']/div/div/div/div/div/div[2]/div/div/article/div/div/table") %>% rvest::html_table(header=T) %>% tidyr::extract(col=`Polling Place`,into="Addr",regex = "(\\d+\\s\\w+\\s\\w+)",remove=F)
pp$Addr[5] <- "494 Lincoln Street"
pp$Addr <- paste(pp$Addr," Waltham",", MA",sep="")
ppll <- geocode(pp$Addr,output = c("latlona"),source="dsk")
ppll <- ppll %>% unique
pp <- inner_join(pp,ppll,by=c("Addr"="address"))
```

```{r 'Graphing Data'}

# ----------------------- Mon Mar 12 19:31:18 2018 ------------------------#
# TODO(Calculate difference between years, and use diverging spectrum to indicate change. Replace 2013 graph with this graph. Change polling point color such that a comparison is not made with k-means graphs.)

# ----------------------- Mon Mar 12 12:20:43 2018 ------------------------#
# 2013 Income
CensusBG.W.Inc %<>% mutate(Diff=Income16-Income13)
ggmap(Waltham)+
  geom_polygon(data=CensusBG.W.Inc,aes(x=long, y=lat, group=group, fill=Diff), size=.2,color='grey', alpha=.7)+
  scale_fill_gradientn(colors=RColorBrewer::brewer.pal(9,"PRGn"))+
  geom_point(data=pp,mapping = aes(x=lon,y=lat))+
  labs(title = "Waltham Married Change in Median Income",
  subtitle = "2013-16 Change in Median Income by Census Block",
  caption = "",
  x = "",y = "") +
  theme(plot.title = element_text(hjust = .5),plot.subtitle = element_text(hjust = .5),axis.text = element_blank(),axis.ticks = element_blank())
ggsave(last_plot(), filename = "IncDiff.pdf", path="Plots",device = "pdf",width = 5, height = 5, units = "in")
# c <- RColorBrewer::brewer.pal(9,"Greens")
# ggmap(Waltham)+
#   geom_polygon(data=CensusBG.W.Inc,aes(x=long, y=lat, group=group, fill=Income13), size=.2,color='grey', alpha=.7)+
#   scale_fill_gradient2(low=c[1],mid=c[5],high=c[9])+
#   geom_point(data=pp,mapping = aes(x=lon,y=lat))+
#   labs(title = "Waltham Married Median Income",
#   subtitle = "2013 Median by Census Block \n Polling Locations in Black",
#   caption = "",
#   x = "",y = "") +
#   theme(plot.title = element_text(hjust = .5),plot.subtitle = element_text(hjust = .5),axis.text = element_blank(),axis.ticks = element_blank())
# ggsave(last_plot(), filename = "Inc13.pdf", path="Plots",device = "pdf",width = 5, height = 5, units = "in")
# ----------------------- Mon Mar 12 12:20:57 2018 ------------------------#
# 2016 Income
c <- RColorBrewer::brewer.pal(9,"Greens")
ggmap(Waltham)+
  geom_polygon(data=CensusBG.W.Inc,aes(x=long, y=lat, group=group, fill=Income16), size=.2,color='grey', alpha=.75)+
  scale_fill_gradientn(colors=RColorBrewer::brewer.pal(9,"Greens"))+
  geom_point(data=pp,mapping = aes(x=lon,y=lat),color=RColorBrewer::brewer.pal(9,"Set1")[2])+
  labs(title = "Proximity to Polling Location by Income",
  subtitle = "2016 Median by Census Block \n Polling Locations in Blue",
  caption = "",
  x = "",y = "") +
  theme(plot.title = element_text(hjust = .5),plot.subtitle = element_text(hjust = .5),axis.text = element_blank(),axis.ticks = element_blank(),plot.margin = margin(rep(0,4),"pt"))
  ggsave(last_plot(), filename = "Inc16.pdf", path="Plots",device = "pdf",width = 5, height = 5, units = "in")

```


